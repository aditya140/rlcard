{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.6.12 64-bit ('rlcard': conda)",
   "display_name": "Python 3.6.12 64-bit ('rlcard': conda)",
   "metadata": {
    "interpreter": {
     "hash": "05f5c1c5a3162497bb8d473e589ac69618e771154a6edb3ca39318199bbc0425"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From /Users/adityaparkhi/DEV/AdvancedMachineLearningCSC722/rlcard/rlcard/utils/utils.py:359: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "  timestep     |  1\n",
      "  reward       |  -1.0\n",
      "----------------------------------------\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'DQN_agent' object has no attribute 'scope'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e993a9be8851>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# Feed transitions into agent memory, and train the agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mts\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrajectories\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# Evaluate the performance. Play with random agents.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DEV/AdvancedMachineLearningCSC722/rlcard/rlcard/agents/my_agent.py\u001b[0m in \u001b[0;36mfeed\u001b[0;34m(self, ts)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_t\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_memory_init_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtmp\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtmp\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DEV/AdvancedMachineLearningCSC722/rlcard/rlcard/agents/my_agent.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     95\u001b[0m         print(\n\u001b[1;32m     96\u001b[0m             \"\\rINFO - Agent {}, step {}, rl-loss: {}\".format(\n\u001b[0;32m---> 97\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m             ),\n\u001b[1;32m     99\u001b[0m             \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DQN_agent' object has no attribute 'scope'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import os\n",
    "\n",
    "import rlcard\n",
    "from rlcard.agents import RandomAgent,DQN_agent,DQN_conf\n",
    "from rlcard.utils import set_global_seed, tournament\n",
    "from rlcard.utils import Logger\n",
    "\n",
    "# Make environment\n",
    "env = rlcard.make('blackjack', config={'seed': 0})\n",
    "eval_env = rlcard.make('blackjack', config={'seed': 0})\n",
    "\n",
    "# Set the iterations numbers and how frequently we evaluate the performance\n",
    "evaluate_every = 100\n",
    "evaluate_num = 1000\n",
    "episode_num = 10000\n",
    "\n",
    "# The intial memory size\n",
    "memory_init_size = 1000\n",
    "\n",
    "# Train the agent every X steps\n",
    "train_every = 1\n",
    "\n",
    "# The paths for saving the logs and learning curves\n",
    "log_dir = './experiments/blackjack_results_dqn/'\n",
    "\n",
    "# Set a global seed\n",
    "set_global_seed(0)\n",
    "\n",
    "params = {\n",
    "    \"scope\":\"DQN-Agent\",\n",
    "    \"num_actions\":env.action_num,\n",
    "    \"replay_memory_size\":memory_init_size,\n",
    "    \"num_states\":env.state_shape,\n",
    "    \"discount_factor\" :0.99,\n",
    "    \"epsilon_start\" : 1.0,\n",
    "    \"epsilon_end\" : 0.1,\n",
    "    \"epsilon_decay_steps\":20000,\n",
    "    \"batch_size\":32,\n",
    "    \"train_every\":1,\n",
    "    \"mlp_layers\":[128,128],\n",
    "    \"lr\":0.1,\n",
    "}\n",
    "\n",
    "agent_conf = DQN_conf(**params)\n",
    "agent = DQN_agent(agent_conf)\n",
    "\n",
    "random_agent = RandomAgent(action_num=eval_env.action_num)\n",
    "env.set_agents([agent, random_agent])\n",
    "eval_env.set_agents([agent, random_agent])\n",
    "\n",
    "logger = Logger(log_dir)\n",
    "\n",
    "for episode in range(episode_num):\n",
    "\n",
    "    # Generate data from the environment\n",
    "    trajectories, _ = env.run(is_training=True)\n",
    "\n",
    "    # Feed transitions into agent memory, and train the agent\n",
    "    for ts in trajectories[0]:\n",
    "        agent.feed(ts)\n",
    "\n",
    "    # Evaluate the performance. Play with random agents.\n",
    "    if episode % evaluate_every == 0:\n",
    "        logger.log_performance(env.timestep, tournament(eval_env, evaluate_num)[0])\n",
    "\n",
    "# Close files in the logger\n",
    "logger.close_files()\n",
    "\n",
    "# Plot the learning curve\n",
    "logger.plot('DQN')\n",
    "\n",
    "# Save model\n",
    "save_dir = 'models/leduc_holdem_dqn_pytorch'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "state_dict = agent.get_state_dict()\n",
    "print(state_dict. keys())\n",
    "torch.save(state_dict, os.path.join(save_dir, 'model.pth'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From /Users/adityaparkhi/DEV/AdvancedMachineLearningCSC722/rlcard/rlcard/utils/utils.py:359: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "  timestep     |  1\n",
      "  reward       |  -0.956\n",
      "----------------------------------------\n",
      "\n",
      "----------------------------------------\n",
      "  timestep     |  133\n",
      "  reward       |  -0.956\n",
      "----------------------------------------\n",
      "\n",
      "----------------------------------------\n",
      "  timestep     |  263\n",
      "  reward       |  -0.952\n",
      "----------------------------------------\n",
      "\n",
      "----------------------------------------\n",
      "  timestep     |  396\n",
      "  reward       |  -0.932\n",
      "----------------------------------------\n",
      "\n",
      "----------------------------------------\n",
      "  timestep     |  539\n",
      "  reward       |  -0.942\n",
      "----------------------------------------\n",
      "\n",
      "----------------------------------------\n",
      "  timestep     |  677\n",
      "  reward       |  -0.952\n",
      "----------------------------------------\n",
      "\n",
      "----------------------------------------\n",
      "  timestep     |  824\n",
      "  reward       |  -0.942\n",
      "----------------------------------------\n",
      "\n",
      "----------------------------------------\n",
      "  timestep     |  968\n",
      "  reward       |  -0.93\n",
      "----------------------------------------\n",
      "[1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 1 0 0 1 1 1 0]\n",
      "INFO - Agent dqn, step 1000, rl-loss: 1.4889277219772339\n",
      "INFO - Copied model parameters to target network.\n",
      "[0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 0]\n",
      "INFO - Agent dqn, step 1001, rl-loss: 0.8521353602409363[0 0 0 0 0 0 0 1 1 1 0 0 0 1 1 1 1 0 0 0 0 1 1 1 0 0 0 1 1 0 1 0]\n",
      "INFO - Agent dqn, step 1002, rl-loss: 0.7826909422874451[0 1 0 0 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 0 1 0 0 0]\n",
      "INFO - Agent dqn, step 1003, rl-loss: 0.6698945164680481[0 0 1 0 1 1 1 0 0 1 1 1 0 1 1 1 0 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0]\n",
      "INFO - Agent dqn, step 1004, rl-loss: 0.8389219641685486[0 1 1 0 0 0 0 1 0 0 0 1 1 0 1 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0 0 0]\n",
      "INFO - Agent dqn, step 1005, rl-loss: 0.6363665461540222[1 1 0 0 0 1 1 0 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 1 0 0 0 1 1 1 0 0]\n",
      "INFO - Agent dqn, step 1006, rl-loss: 0.7091376781463623[1 0 0 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 0 0 1 0 0 1 1 1 1 1 0 1 0 0]\n",
      "INFO - Agent dqn, step 1007, rl-loss: 0.9690213799476624[1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 1]\n",
      "INFO - Agent dqn, step 1008, rl-loss: 0.854042649269104[1 0 0 0 1 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0]\n",
      "INFO - Agent dqn, step 1009, rl-loss: 0.6183761954307556[1 1 1 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 1 0 1 0 1 1 0 1]\n",
      "INFO - Agent dqn, step 1010, rl-loss: 0.5498539209365845[0 1 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 1 0 0]\n",
      "INFO - Agent dqn, step 1011, rl-loss: 0.7044442892074585[1 0 1 0 0 0 0 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1]\n",
      "INFO - Agent dqn, step 1012, rl-loss: 0.6651546359062195[0 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 1 0 1]\n",
      "INFO - Agent dqn, step 1013, rl-loss: 0.5733888745307922[0 0 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 0 1]\n",
      "INFO - Agent dqn, step 1014, rl-loss: 0.7237485647201538[0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1]\n",
      "INFO - Agent dqn, step 1015, rl-loss: 0.6762591004371643[0 1 0 0 0 1 0 0 1 1 1 0 1 0 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 1 1 1]\n",
      "INFO - Agent dqn, step 1016, rl-loss: 0.6800475120544434[0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 1 1 1 1 0 1 0 1 0 0 0 0 1 0 0]\n",
      "INFO - Agent dqn, step 1017, rl-loss: 0.620387613773346[1 1 0 0 1 0 1 0 0 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 1 0 0]\n",
      "INFO - Agent dqn, step 1018, rl-loss: 0.7466610074043274[1 0 0 0 0 0 0 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0]\n",
      "INFO - Agent dqn, step 1019, rl-loss: 0.5525362491607666[1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 0 0 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0]\n",
      "INFO - Agent dqn, step 1020, rl-loss: 0.5688605308532715[0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 0 0 0 1 1 1 1 1 0 0]\n",
      "INFO - Agent dqn, step 1021, rl-loss: 0.6233121156692505[0 1 0 1 1 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 1 0 1 0]\n",
      "INFO - Agent dqn, step 1022, rl-loss: 0.5300639271736145[0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0]\n",
      "INFO - Agent dqn, step 1023, rl-loss: 0.4989025592803955[1 0 1 1 1 0 0 1 1 1 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 0]\n",
      "INFO - Agent dqn, step 1024, rl-loss: 0.525355339050293[0 1 1 0 0 1 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0]\n",
      "INFO - Agent dqn, step 1025, rl-loss: 0.5382753014564514[0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 1 1 0 0 0 0 1]\n",
      "INFO - Agent dqn, step 1026, rl-loss: 0.7582067251205444[1 1 0 1 0 0 1 1 1 1 0 0 0 0 1 1 0 0 1 1 1 0 1 0 0 1 0 0 0 0 1 1]\n",
      "INFO - Agent dqn, step 1027, rl-loss: 0.7428274750709534[1 1 1 0 1 1 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 1 1 0 1 1 0 1 0 1 0]\n",
      "INFO - Agent dqn, step 1028, rl-loss: 0.6111583709716797[1 0 0 1 0 1 1 1 1 1 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1]\n",
      "INFO - Agent dqn, step 1029, rl-loss: 0.6700899600982666[0 1 1 0 0 1 0 1 0 1 0 0 1 1 1 0 0 1 1 1 0 1 0 1 1 1 0 0 1 1 1 0]\n",
      "INFO - Agent dqn, step 1030, rl-loss: 0.7458704710006714[0 0 1 1 1 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1]\n",
      "INFO - Agent dqn, step 1031, rl-loss: 0.8766028881072998[0 1 1 0 1 1 1 0 0 1 1 1 0 0 1 0 1 1 1 1 0 0 1 1 0 1 1 0 1 0 1 1]\n",
      "INFO - Agent dqn, step 1032, rl-loss: 0.8160234689712524[1 0 0 1 0 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 1 0 0 0]\n",
      "INFO - Agent dqn, step 1033, rl-loss: 0.6175833344459534[1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 0 0 0 1 0 1 1 1 1 1 1 1 1 0 1 1 0]\n",
      "INFO - Agent dqn, step 1034, rl-loss: 0.7145702838897705[0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 1 0 1 1 1 0 0 1 0]\n",
      "INFO - Agent dqn, step 1035, rl-loss: 0.41347217559814453[1 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0 1]\n",
      "INFO - Agent dqn, step 1036, rl-loss: 0.535849392414093[1 0 1 1 0 0 1 1 0 1 1 1 0 0 1 0 0 1 0 1 0 0 0 0 1 1 0 1 0 1 0 0]\n",
      "INFO - Agent dqn, step 1037, rl-loss: 0.5756834745407104[0 1 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0]\n",
      "INFO - Agent dqn, step 1038, rl-loss: 0.6616599559783936[0 1 0 1 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1]\n",
      "INFO - Agent dqn, step 1039, rl-loss: 0.5385289788246155[1 1 1 1 1 0 1 0 1 1 0 0 1 1 0 0 0 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0]\n",
      "INFO - Agent dqn, step 1040, rl-loss: 0.5530967116355896[1 0 1 0 0 1 1 0 1 1 1 0 1 1 0 1 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1]\n",
      "INFO - Agent dqn, step 1041, rl-loss: 0.6227893233299255[1 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 1 1 0 1 1 0 0]\n",
      "INFO - Agent dqn, step 1042, rl-loss: 0.5881760120391846[1 0 1 0 0 1 1 0 1 0 0 0 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0]\n",
      "INFO - Agent dqn, step 1043, rl-loss: 0.7871290445327759[0 0 0 1 1 0 0 1 0 0 1 0 1 0 1 0 0 1 0 1 1 0 0 0 1 1 0 0 1 0 0 1]\n",
      "INFO - Agent dqn, step 1044, rl-loss: 0.4933832585811615[0 1 0 0 1 1 1 0 0 0 1 1 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0]\n",
      "INFO - Agent dqn, step 1045, rl-loss: 0.4655308127403259[0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 1 0 0 0 0 1 0 1 0 0 1 1 1 1 0 0 1]\n",
      "INFO - Agent dqn, step 1046, rl-loss: 0.5806816220283508[1 1 1 1 0 1 0 1 0 1 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 0 1 0 1 1 1]\n",
      "INFO - Agent dqn, step 1047, rl-loss: 0.6316541433334351[1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 0 0 0 0 0 1]\n",
      "INFO - Agent dqn, step 1048, rl-loss: 0.6555485725402832[1 0 0 1 0 0 1 1 1 0 0 1 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 1 1 0]\n",
      "INFO - Agent dqn, step 1049, rl-loss: 0.6666958928108215[1 1 0 1 1 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 1]\n",
      "INFO - Agent dqn, step 1050, rl-loss: 0.6102077960968018[0 1 1 1 1 1 0 1 0 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 0 1]\n",
      "INFO - Agent dqn, step 1051, rl-loss: 0.6113678216934204[1 1 0 0 0 1 1 1 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0]\n",
      "INFO - Agent dqn, step 1052, rl-loss: 0.4950559735298157[1 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 1 1 0 1 0 0 0]\n",
      "INFO - Agent dqn, step 1053, rl-loss: 0.5414901971817017[1 1 0 0 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 0 1 1 1 1 1 0 1 0 0 0]\n",
      "INFO - Agent dqn, step 1054, rl-loss: 0.5535640120506287[0 0 1 1 0 0 1 0 0 1 1 1 1 0 0 0 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1]\n",
      "INFO - Agent dqn, step 1055, rl-loss: 0.6787723302841187[1 1 1 0 1 0 0 0 1 1 0 0 1 1 0 0 0 0 1 1 0 1 1 1 1 1 0 0 0 1 1 1]\n",
      "INFO - Agent dqn, step 1056, rl-loss: 0.7106044888496399[1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 1 0 0 0 0 1 1 1]\n",
      "INFO - Agent dqn, step 1057, rl-loss: 0.6122767329216003[0 0 1 1 0 1 0 1 0 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0]\n",
      "INFO - Agent dqn, step 1058, rl-loss: 0.6977778077125549[1 1 0 1 1 0 1 1 1 0 1 1 1 0 0 0 0 0 0 1 1 1 1 0 0 1 0 0 1 0 1 0]\n",
      "INFO - Agent dqn, step 1059, rl-loss: 0.6312983632087708[0 1 0 1 1 0 0 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 1 0 1 0 0]\n",
      "INFO - Agent dqn, step 1060, rl-loss: 0.5098496675491333[1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 0 1 1 0 1 1 1 1 0 1 0 0 1 0 1 1]\n",
      "INFO - Agent dqn, step 1061, rl-loss: 0.6732104420661926[0 0 0 1 1 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 1 0 0]\n",
      "INFO - Agent dqn, step 1062, rl-loss: 0.4798685908317566[1 1 0 1 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 1 0 0 0 0 1 0 0 1 1 0 0 1]\n",
      "INFO - Agent dqn, step 1063, rl-loss: 0.4628591537475586[1 1 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 1 0 0 1 0]\n",
      "INFO - Agent dqn, step 1064, rl-loss: 0.4687403440475464[0 0 1 1 1 0 1 1 0 0 0 1 0 0 0 1 0 0 0 1 1 1 0 0 1 1 1 1 1 0 0 1]\n",
      "INFO - Agent dqn, step 1065, rl-loss: 0.5889575481414795[1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 0 0]\n",
      "INFO - Agent dqn, step 1066, rl-loss: 0.5033791661262512[1 1 0 0 0 1 0 0 0 1 0 0 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 1 1 1 0 0]\n",
      "INFO - Agent dqn, step 1067, rl-loss: 0.5124215483665466[1 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0]\n",
      "INFO - Agent dqn, step 1068, rl-loss: 0.5482351183891296[0 0 1 0 0 1 0 1 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 1]\n",
      "INFO - Agent dqn, step 1069, rl-loss: 0.4829224944114685[0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 1 1 0 0 1 0 1 1 1 1 0 0 0]\n",
      "INFO - Agent dqn, step 1070, rl-loss: 0.5897818207740784[1 0 0 1 1 0 1 0 0 0 0 0 0 1 1 0 1 0 0 1 1 0 0 1 1 0 0 0 1 0 0 1]\n",
      "INFO - Agent dqn, step 1071, rl-loss: 0.5026780962944031[0 0 0 1 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 1 0 0 1]\n",
      "INFO - Agent dqn, step 1072, rl-loss: 0.3975401222705841[1 1 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 1 1 0]\n",
      "INFO - Agent dqn, step 1073, rl-loss: 0.609282910823822[1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 1 1 1 1]\n",
      "INFO - Agent dqn, step 1074, rl-loss: 0.6729698181152344[0 1 1 0 1 0 1 0 1 0 0 1 0 0 0 1 0 1 1 1 0 0 1 0 1 0 1 0 1 0 1 0]\n",
      "INFO - Agent dqn, step 1075, rl-loss: 0.5551681518554688[1 0 0 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1]\n",
      "INFO - Agent dqn, step 1076, rl-loss: 0.5482925772666931[1 1 1 1 1 0 1 0 0 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 1 1 1 1 1 1 1]\n",
      "INFO - Agent dqn, step 1077, rl-loss: 0.6780194044113159[0 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1 1 0]\n",
      "INFO - Agent dqn, step 1078, rl-loss: 0.6448936462402344[0 0 1 0 1 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 0]\n",
      "INFO - Agent dqn, step 1079, rl-loss: 0.5836502313613892[0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 1 0 0 0 1 1 0 0 1 0 1]\n",
      "INFO - Agent dqn, step 1080, rl-loss: 0.4903179109096527[0 0 0 0 1 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 1 1 1 0 0 1 1 0 0 0 0]\n",
      "INFO - Agent dqn, step 1081, rl-loss: 0.44164517521858215[0 1 0 1 1 0 1 0 1 0 1 1 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1]\n",
      "INFO - Agent dqn, step 1082, rl-loss: 0.5908270478248596[0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 0 0 1]\n",
      "INFO - Agent dqn, step 1083, rl-loss: 0.49723729491233826[0 1 1 1 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 0 1 1 1 1 0 1 1 1 1 1 0 0]\n",
      "INFO - Agent dqn, step 1084, rl-loss: 0.5489767789840698[1 1 0 1 0 0 0 1 0 1 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 1 1 0 0 1 0 1]\n",
      "INFO - Agent dqn, step 1085, rl-loss: 0.524166464805603[0 1 1 0 1 0 1 0 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0]\n",
      "INFO - Agent dqn, step 1086, rl-loss: 0.5549049973487854[0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 1 1 0 0 0]\n",
      "INFO - Agent dqn, step 1087, rl-loss: 0.51222163438797[0 1 1 1 0 0 1 1 0 0 0 0 0 1 1 1 0 0 0 1 1 1 0 0 1 1 0 1 1 0 1 1]\n",
      "INFO - Agent dqn, step 1088, rl-loss: 0.6518604755401611[1 0 1 0 0 0 0 0 0 0 0 1 1 1 0 1 1 1 0 1 1 1 1 1 0 0 1 0 1 1 1 1]\n",
      "INFO - Agent dqn, step 1089, rl-loss: 0.6330540776252747[0 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 1 0 0 0 1 1 0 0 0 1 0]\n",
      "INFO - Agent dqn, step 1090, rl-loss: 0.45953652262687683[0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 0 0 1 0 1]\n",
      "INFO - Agent dqn, step 1091, rl-loss: 0.4373895823955536[1 1 1 0 1 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 1 0 1 1 1 0 0 0 1]\n",
      "INFO - Agent dqn, step 1092, rl-loss: 0.5843603610992432[1 0 1 0 0 0 1 0 1 1 1 0 0 1 0 1 0 1 0 1 1 0 0 1 0 0 1 0 0 1 1 1]\n",
      "INFO - Agent dqn, step 1093, rl-loss: 0.5826011896133423[1 0 1 1 1 0 0 0 0 0 1 1 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 1 0 1 0]\n",
      "INFO - Agent dqn, step 1094, rl-loss: 0.510495126247406[1 0 0 1 0 0 1 0 1 0 0 0 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 0 0 0 0 1]\n",
      "INFO - Agent dqn, step 1095, rl-loss: 0.48435109853744507[0 0 1 1 0 1 0 0 0 0 0 0 1 1 1 1 0 1 0 1 0 0 0 1 0 1 1 0 1 1 0 0]\n",
      "INFO - Agent dqn, step 1096, rl-loss: 0.6205103397369385[1 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      "INFO - Agent dqn, step 1097, rl-loss: 0.37567955255508423[0 0 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 1 1 0]\n",
      "INFO - Agent dqn, step 1098, rl-loss: 0.4350168704986572[1 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 0 1 1 1 0 0 1 0]\n",
      "INFO - Agent dqn, step 1099, rl-loss: 0.5577661395072937[1 0 0 0 1 0 1 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 0 0 0 1 1 0 0 1 0 0]\n",
      "INFO - Agent dqn, step 1100, rl-loss: 0.5078876614570618[1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 1 1 1 0 0 0 0 1 1 1 0 1 1 1 1 0 0]\n",
      "INFO - Agent dqn, step 1101, rl-loss: 0.6112182140350342[1 1 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 0 0 1 1 0 1 0 1 0 0 0 0]\n",
      "INFO - Agent dqn, step 1102, rl-loss: 0.4517279267311096[0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0]\n",
      "INFO - Agent dqn, step 1103, rl-loss: 0.3966609239578247[1 1 1 1 0 0 1 1 1 0 1 0 0 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 1 0 1 0]\n",
      "INFO - Agent dqn, step 1104, rl-loss: 0.6131435632705688[0 0 1 0 1 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1]\n",
      "INFO - Agent dqn, step 1105, rl-loss: 0.5532385110855103[0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 1 1 1 0]\n",
      "INFO - Agent dqn, step 1106, rl-loss: 0.44019511342048645[0 1 1 0 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 0 1 1]\n",
      "INFO - Agent dqn, step 1107, rl-loss: 0.5309429168701172\n",
      "----------------------------------------\n",
      "  timestep     |  1107\n",
      "  reward       |  -0.173\n",
      "----------------------------------------\n",
      "[0 1 1 0 0 0 1 1 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1]\n",
      "INFO - Agent dqn, step 1108, rl-loss: 0.5062215924263[1 0 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0]\n",
      "INFO - Agent dqn, step 1109, rl-loss: 0.6569452285766602[1 0 0 0 0 1 0 0 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 0 0 0 0 1 0 1 0 1]\n",
      "INFO - Agent dqn, step 1110, rl-loss: 0.42568907141685486[1 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 1 1 1]\n",
      "INFO - Agent dqn, step 1111, rl-loss: 0.45730510354042053[0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0]\n",
      "INFO - Agent dqn, step 1112, rl-loss: 0.5128981471061707[0 0 1 1 0 1 0 0 1 0 0 1 1 0 0 1 1 1 1 0 0 0 0 1 0 1 0 0 0 0 1 0]\n",
      "INFO - Agent dqn, step 1113, rl-loss: 0.5203052759170532[0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 1]\n",
      "INFO - Agent dqn, step 1114, rl-loss: 0.4627785086631775[0 0 1 1 1 0 1 0 0 1 1 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 0 1]\n",
      "INFO - Agent dqn, step 1115, rl-loss: 0.47697022557258606[1 0 1 0 0 1 0 0 0 0 0 0 1 0 1 0 0 1 1 1 1 0 1 0 0 0 0 1 1 1 0 0]\n",
      "INFO - Agent dqn, step 1116, rl-loss: 0.39665624499320984[1 0 0 0 0 0 1 0 1 1 0 0 1 1 1 0 1 1 1 0 0 0 0 1 0 1 1 1 1 1 1 0]\n",
      "INFO - Agent dqn, step 1117, rl-loss: 0.4840308725833893[0 1 0 0 1 0 1 0 1 1 1 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0]\n",
      "INFO - Agent dqn, step 1118, rl-loss: 0.47593531012535095[1 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 1 0 1 1 1 1 1 0 0 0]\n",
      "INFO - Agent dqn, step 1119, rl-loss: 0.4163094162940979[0 1 1 1 1 1 0 1 0 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 1 0 1 1 1]\n",
      "INFO - Agent dqn, step 1120, rl-loss: 0.5345409512519836[0 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 1 0 1 0 1 0 0 0 0 1 0 1 1]\n",
      "INFO - Agent dqn, step 1121, rl-loss: 0.47348374128341675[1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 0 0 0 1 1 1 1 1 1 1 1 0 1]\n",
      "INFO - Agent dqn, step 1122, rl-loss: 0.5884775519371033[0 0 0 1 1 0 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 1 1 1 1]\n",
      "INFO - Agent dqn, step 1123, rl-loss: 0.48560309410095215[0 1 0 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 0 0 1 0 1 0 1 0 1 1 1]\n",
      "INFO - Agent dqn, step 1124, rl-loss: 0.5110062956809998[0 0 1 1 0 0 0 0 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1 0]\n",
      "INFO - Agent dqn, step 1125, rl-loss: 0.550543487071991[1 1 1 1 0 0 0 1 0 1 1 0 0 1 1 0 1 0 0 1 1 0 0 0 1 0 1 0 0 0 1 1]\n",
      "INFO - Agent dqn, step 1126, rl-loss: 0.572967529296875[0 0 1 0 0 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 1 1 0 1 1 1 0 1 0 1 0 0]\n",
      "INFO - Agent dqn, step 1127, rl-loss: 0.4983491003513336[0 1 0 1 1 0 0 0 1 1 0 1 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0 1 1]\n",
      "INFO - Agent dqn, step 1128, rl-loss: 0.5322111248970032[0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1 0 1 1 0 1 1 0 1 0 0 0 1 0 0]\n",
      "INFO - Agent dqn, step 1129, rl-loss: 0.5953637361526489[0 0 0 0 1 1 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0 1 0 0 0 0 0 0 0 1 1 0]\n",
      "INFO - Agent dqn, step 1130, rl-loss: 0.38731318712234497[1 0 0 0 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 1]\n",
      "INFO - Agent dqn, step 1131, rl-loss: 0.5077852010726929[1 0 1 0 0 1 1 1 0 0 0 1 0 1 0 1 1 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0]\n",
      "INFO - Agent dqn, step 1132, rl-loss: 0.46487197279930115[0 1 1 1 1 1 0 0 0 1 0 1 1 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0]\n",
      "INFO - Agent dqn, step 1133, rl-loss: 0.48608577251434326[1 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 1 1 1 1 0 1 0]\n",
      "INFO - Agent dqn, step 1134, rl-loss: 0.6061860918998718[1 0 0 0 0 0 1 0 0 1 0 1 0 0 0 1 1 0 1 1 1 1 0 0 1 1 1 0 1 0 0 0]\n",
      "INFO - Agent dqn, step 1135, rl-loss: 0.48942458629608154[0 1 1 0 1 1 0 0 1 1 1 1 0 0 1 1 1 0 1 0 0 1 1 0 1 0 1 1 1 1 1 1]\n",
      "INFO - Agent dqn, step 1136, rl-loss: 0.6178995966911316[1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 1 1 0 0 0 0 1 0 0 0 1 0 0 1]\n",
      "INFO - Agent dqn, step 1137, rl-loss: 0.45392757654190063[1 0 1 0 1 1 1 1 0 0 1 1 1 0 1 0 1 0 0 1 1 1 0 0 0 1 0 0 1 0 0 0]\n",
      "INFO - Agent dqn, step 1138, rl-loss: 0.6593442559242249[0 0 1 1 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0]\n",
      "INFO - Agent dqn, step 1139, rl-loss: 0.509641170501709[0 0 0 0 0 0 1 1 1 0 0 0 1 1 1 1 1 1 0 0 1 0 0 1 1 0 1 1 0 0 0 0]\n",
      "INFO - Agent dqn, step 1140, rl-loss: 0.44373050332069397[1 1 0 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 0 1 1 1 0 1 1 1]\n",
      "INFO - Agent dqn, step 1141, rl-loss: 0.5529550313949585[1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 1 1 1]\n",
      "INFO - Agent dqn, step 1142, rl-loss: 0.4248056411743164[1 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 1 1]\n",
      "INFO - Agent dqn, step 1143, rl-loss: 0.4240019917488098[1 0 0 1 0 0 0 0 1 1 0 1 0 0 1 1 0 1 0 1 1 1 0 1 1 0 0 1 1 1 1 0]\n",
      "INFO - Agent dqn, step 1144, rl-loss: 0.5363246202468872[1 0 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1 0 1 1]\n",
      "INFO - Agent dqn, step 1145, rl-loss: 0.5294560194015503[0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 0 0 1 1 1 0 0 1 1 1 0 0 1 1 0 0]\n",
      "INFO - Agent dqn, step 1146, rl-loss: 0.4807858467102051[0 0 1 1 0 1 0 1 1 1 0 0 1 1 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0]\n",
      "INFO - Agent dqn, step 1147, rl-loss: 0.5289413928985596[1 0 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1]\n",
      "INFO - Agent dqn, step 1148, rl-loss: 0.583357572555542[1 1 0 1 1 1 0 1 0 1 1 1 1 1 0 0 0 1 1 1 0 0 1 0 1 0 1 0 1 1 0 1]\n",
      "INFO - Agent dqn, step 1149, rl-loss: 0.6826830506324768[1 0 0 0 1 1 0 1 0 0 1 0 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 0 0]\n",
      "INFO - Agent dqn, step 1150, rl-loss: 0.451164573431015[0 1 1 0 1 0 1 1 1 1 0 0 1 0 1 0 1 0 1 0 0 0 0 1 0 1 1 1 0 1 0 1]\n",
      "INFO - Agent dqn, step 1151, rl-loss: 0.6194134950637817[1 1 1 0 0 0 0 1 1 1 0 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 1 1 1]\n",
      "INFO - Agent dqn, step 1152, rl-loss: 0.4186740219593048[1 1 1 1 0 0 0 0 0 0 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 0 1 1 0 0 1]\n",
      "INFO - Agent dqn, step 1153, rl-loss: 0.48483389616012573[0 0 0 1 0 1 1 0 0 0 1 1 1 0 0 1 1 1 0 1 0 1 1 0 0 0 0 1 0 1 1 0]\n",
      "INFO - Agent dqn, step 1154, rl-loss: 0.5554245114326477[0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1 1 0 1 0 1 0 0 1]\n",
      "INFO - Agent dqn, step 1155, rl-loss: 0.5524541735649109[0 0 1 1 0 1 1 0 0 0 1 0 0 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1]\n",
      "INFO - Agent dqn, step 1156, rl-loss: 0.6422513127326965[0 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 1 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1]\n",
      "INFO - Agent dqn, step 1157, rl-loss: 0.50107741355896[1 1 1 0 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 1 0 0 1]\n",
      "INFO - Agent dqn, step 1158, rl-loss: 0.5577671527862549[0 1 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 1 1 0 1 1]\n",
      "INFO - Agent dqn, step 1159, rl-loss: 0.3959024250507355[0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 0 1 0 0]\n",
      "INFO - Agent dqn, step 1160, rl-loss: 0.35732582211494446[1 1 1 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 0 1]\n",
      "INFO - Agent dqn, step 1161, rl-loss: 0.4423672556877136[1 1 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 0 0 0 1 0 0 0]\n",
      "INFO - Agent dqn, step 1162, rl-loss: 0.5352302193641663[1 0 1 0 0 0 0 1 0 1 1 1 0 0 0 1 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0 1]\n",
      "INFO - Agent dqn, step 1163, rl-loss: 0.5970849990844727[1 0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 0 0 1 1 0]\n",
      "INFO - Agent dqn, step 1164, rl-loss: 0.4534841477870941[1 1 0 0 0 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 0 1]\n",
      "INFO - Agent dqn, step 1165, rl-loss: 0.5383309125900269[1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 0 1 0 0 1]\n",
      "INFO - Agent dqn, step 1166, rl-loss: 0.552773654460907[0 0 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0]\n",
      "INFO - Agent dqn, step 1167, rl-loss: 0.39518630504608154[1 1 1 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 0 1 0 1 0]\n",
      "INFO - Agent dqn, step 1168, rl-loss: 0.5220471024513245[0 1 0 0 0 1 1 1 0 0 0 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 0 0 1 0 1 0]\n",
      "INFO - Agent dqn, step 1169, rl-loss: 0.3361283838748932[0 1 0 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 1 0 0 0 1 1 1 1 0 1 1 1 1 1]\n",
      "INFO - Agent dqn, step 1170, rl-loss: 0.484308123588562[1 0 0 0 0 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 1 1 1 0 0 1 0]\n",
      "INFO - Agent dqn, step 1171, rl-loss: 0.3767837882041931[0 1 0 0 1 0 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 1 0 0]\n",
      "INFO - Agent dqn, step 1172, rl-loss: 0.37451449036598206[0 1 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 1 0 1 0 1 1]\n",
      "INFO - Agent dqn, step 1173, rl-loss: 0.4555874466896057[0 1 1 1 0 1 0 0 0 1 1 1 0 0 1 0 1 1 1 1 1 0 0 0 0 0 0 0 1 1 1 0]\n",
      "INFO - Agent dqn, step 1174, rl-loss: 0.5644301176071167[0 0 1 0 0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0]\n",
      "INFO - Agent dqn, step 1175, rl-loss: 0.43791013956069946[1 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 0 1 1 1 1 0 1 1 1 1]\n",
      "INFO - Agent dqn, step 1176, rl-loss: 0.6324260830879211[0 1 1 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 1 1 1 1 1]\n",
      "INFO - Agent dqn, step 1177, rl-loss: 0.41110920906066895[0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 1 1 0 1 1 1 0 1 0 0 1 1 0 1 1 0 0]\n",
      "INFO - Agent dqn, step 1178, rl-loss: 0.4800994396209717[0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1 0]\n",
      "INFO - Agent dqn, step 1179, rl-loss: 0.5788799524307251[0 1 1 0 0 0 1 0 0 0 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 1 0 0 0]\n",
      "INFO - Agent dqn, step 1180, rl-loss: 0.3997453451156616[0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1 1 1 0 1 0 1]\n",
      "INFO - Agent dqn, step 1181, rl-loss: 0.3822110891342163[1 1 1 0 0 1 0 1 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 0 1 0 0 0 1 1]\n",
      "INFO - Agent dqn, step 1182, rl-loss: 0.46098563075065613[0 1 1 1 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 1 0 1 0 0 1 0 0 1 1 1 1 1]\n",
      "INFO - Agent dqn, step 1183, rl-loss: 0.4158323109149933[1 1 1 0 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 1 0 0 0 0 1 1 0 1 1]\n",
      "INFO - Agent dqn, step 1184, rl-loss: 0.5287249088287354[0 0 1 1 1 0 1 0 1 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0]\n",
      "INFO - Agent dqn, step 1185, rl-loss: 0.40373092889785767[1 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1]\n",
      "INFO - Agent dqn, step 1186, rl-loss: 0.44766396284103394[1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 0 0 1]\n",
      "INFO - Agent dqn, step 1187, rl-loss: 0.6850128769874573[1 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 1 0]\n",
      "INFO - Agent dqn, step 1188, rl-loss: 0.4427908957004547[1 0 0 1 0 0 0 0 1 1 1 1 1 0 1 0 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 0]\n",
      "INFO - Agent dqn, step 1189, rl-loss: 0.45891740918159485[1 1 0 0 1 1 0 0 0 0 1 1 0 1 0 0 1 0 1 1 0 1 1 1 0 0 1 0 0 1 0 0]\n",
      "INFO - Agent dqn, step 1190, rl-loss: 0.40118879079818726[0 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1 0 0]\n",
      "INFO - Agent dqn, step 1191, rl-loss: 0.4587891101837158[0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 1 1 1 0 1 1 1 0 1 1 0 0 0]\n",
      "INFO - Agent dqn, step 1192, rl-loss: 0.5197304487228394[0 0 0 1 0 0 0 1 1 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 0 0 1 1 0 0 1]\n",
      "INFO - Agent dqn, step 1193, rl-loss: 0.43295514583587646[1 0 0 0 0 0 0 1 0 0 1 1 1 1 0 0 1 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1]\n",
      "INFO - Agent dqn, step 1194, rl-loss: 0.5507755875587463[0 0 0 0 0 1 1 0 0 0 0 1 0 1 1 0 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0 1]\n",
      "INFO - Agent dqn, step 1195, rl-loss: 0.5869770050048828[0 1 1 1 0 1 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1 0]\n",
      "INFO - Agent dqn, step 1196, rl-loss: 0.3658699691295624[1 1 1 0 1 1 1 1 0 1 1 1 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 0]\n",
      "INFO - Agent dqn, step 1197, rl-loss: 0.6094299554824829[1 0 1 1 0 1 1 0 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1]\n",
      "INFO - Agent dqn, step 1198, rl-loss: 0.43144041299819946[0 0 1 0 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 1 1 0 0 0 0 0]\n",
      "INFO - Agent dqn, step 1199, rl-loss: 0.4830918312072754[0 1 1 0 1 0 0 0 1 1 0 1 1 0 1 1 0 0 1 0 0 0 0 1 1 1 0 1 1 0 0 1]\n",
      "INFO - Agent dqn, step 1200, rl-loss: 0.37802839279174805[1 1 0 0 0 1 1 0 1 0 0 0 0 1 1 1 0 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1]\n",
      "INFO - Agent dqn, step 1201, rl-loss: 0.44057220220565796[0 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0 1 1 0 0 1 0 1 1 1 0 0 1 0 1 0]\n",
      "INFO - Agent dqn, step 1202, rl-loss: 0.4027344584465027[0 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1 0 0 0 0 1 0]\n",
      "INFO - Agent dqn, step 1203, rl-loss: 0.29893845319747925[0 0 0 1 0 0 0 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 1 0 1 1]\n",
      "INFO - Agent dqn, step 1204, rl-loss: 0.4552752375602722[1 1 0 0 1 1 1 1 0 0 1 0 1 0 1 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1]\n",
      "INFO - Agent dqn, step 1205, rl-loss: 0.4320203363895416[0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 1 1 0 1 1 1 1 1 1 0 1]\n",
      "INFO - Agent dqn, step 1206, rl-loss: 0.4785713851451874[0 1 0 1 0 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 1 1]\n",
      "INFO - Agent dqn, step 1207, rl-loss: 0.5938827991485596[1 1 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 0 1 1 0 1 1]\n",
      "INFO - Agent dqn, step 1208, rl-loss: 0.5265692472457886[0 1 1 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 1 1 1 0]\n",
      "INFO - Agent dqn, step 1209, rl-loss: 0.393137663602829[0 0 1 0 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 1 1 0 0 0 1 1 0 0 0 1]\n",
      "INFO - Agent dqn, step 1210, rl-loss: 0.5434089303016663[1 1 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 0 0 0 0]\n",
      "INFO - Agent dqn, step 1211, rl-loss: 0.49293941259384155[1 0 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1]\n",
      "INFO - Agent dqn, step 1212, rl-loss: 0.4221631586551666[0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 1 0 0 0 0 0 1]\n",
      "INFO - Agent dqn, step 1213, rl-loss: 0.3737005293369293[1 0 0 0 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0]\n",
      "INFO - Agent dqn, step 1214, rl-loss: 0.5697809457778931[1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 1 0 0 0 0 0 0 1 1 1 0]\n",
      "INFO - Agent dqn, step 1215, rl-loss: 0.4181540906429291[1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 0 1 1 0 1 0 0 1 1 0 1]\n",
      "INFO - Agent dqn, step 1216, rl-loss: 0.7974932193756104[0 0 1 1 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 0 0 1 1 0 0]\n",
      "INFO - Agent dqn, step 1217, rl-loss: 0.5275061130523682[0 1 0 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 0 1]\n",
      "INFO - Agent dqn, step 1218, rl-loss: 0.5318214893341064[1 0 0 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 0 1 1]\n",
      "INFO - Agent dqn, step 1219, rl-loss: 0.3649336099624634[1 0 1 1 1 1 1 0 1 0 0 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 0 0 1 1 0 0]\n",
      "INFO - Agent dqn, step 1220, rl-loss: 0.5296441316604614[1 0 0 1 1 1 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1 1 1 0 0 0 1 0 1 0 0]\n",
      "INFO - Agent dqn, step 1221, rl-loss: 0.47714510560035706[1 0 1 1 1 1 1 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1 0 0]\n",
      "INFO - Agent dqn, step 1222, rl-loss: 0.4154832065105438[0 0 0 1 1 1 0 0 0 1 0 0 0 0 1 1 0 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1]\n",
      "INFO - Agent dqn, step 1223, rl-loss: 0.5902687907218933[0 1 0 1 0 0 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 1 1 1 0]\n",
      "INFO - Agent dqn, step 1224, rl-loss: 0.34733670949935913[1 0 1 0 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 0 1 0 1 0 0 1 1 0 1 1 1 1]\n",
      "INFO - Agent dqn, step 1225, rl-loss: 0.3885098099708557[1 0 0 1 1 0 1 0 1 1 1 0 1 1 1 0 1 1 1 0 0 1 0 1 0 0 0 1 0 1 1 0]\n",
      "INFO - Agent dqn, step 1226, rl-loss: 0.4978819787502289[1 1 1 0 1 0 1 0 1 1 1 0 0 1 1 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0]\n",
      "INFO - Agent dqn, step 1227, rl-loss: 0.3934747874736786[1 0 1 1 1 1 0 1 0 1 0 0 1 1 0 1 0 0 1 1 0 1 1 0 0 1 1 1 0 0 1 1]\n",
      "INFO - Agent dqn, step 1228, rl-loss: 0.479204922914505[0 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1]\n",
      "INFO - Agent dqn, step 1229, rl-loss: 0.45563334226608276[1 0 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 1 1 0 1 1 1 0 1 1 1]\n",
      "INFO - Agent dqn, step 1230, rl-loss: 0.5076133012771606[1 1 0 1 0 0 1 1 0 0 0 0 1 1 0 0 1 0 1 1 0 0 0 0 0 0 0 0 1 1 0 0]\n",
      "INFO - Agent dqn, step 1231, rl-loss: 0.35222554206848145[1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0]\n",
      "INFO - Agent dqn, step 1232, rl-loss: 0.3854316473007202[0 1 0 0 0 0 0 1 0 0 0 1 1 0 1 0 1 0 0 0 1 0 0 0 1 0 1 1 1 1 1 0]\n",
      "INFO - Agent dqn, step 1233, rl-loss: 0.47525185346603394[0 1 1 1 1 0 0 0 0 0 1 1 1 1 0 0 1 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1]\n",
      "INFO - Agent dqn, step 1234, rl-loss: 0.39921167492866516[1 0 1 1 1 1 0 0 0 0 0 1 0 0 0 1 0 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1]\n",
      "INFO - Agent dqn, step 1235, rl-loss: 0.5898118615150452[0 1 1 0 1 1 1 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "INFO - Agent dqn, step 1236, rl-loss: 0.3878220021724701[0 1 1 0 1 1 1 1 1 0 1 0 1 0 0 0 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0]\n",
      "INFO - Agent dqn, step 1237, rl-loss: 0.5939061045646667[0 1 0 0 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 0 0 1 1 0 1 0 0 1]\n",
      "INFO - Agent dqn, step 1238, rl-loss: 0.4839005470275879[1 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 0 1 0 0 1 1 0 0 0 0 1 1]\n",
      "INFO - Agent dqn, step 1239, rl-loss: 0.38949671387672424[0 1 1 0 1 1 1 0 0 1 1 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0]\n",
      "INFO - Agent dqn, step 1240, rl-loss: 0.41443243622779846[1 0 0 0 0 0 1 1 0 1 0 0 0 1 1 0 0 1 1 0 0 0 1 1 0 1 0 0 0 1 0 1]\n",
      "INFO - Agent dqn, step 1241, rl-loss: 0.41831594705581665[1 0 1 0 1 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 1 0 0 0 0 0 0]\n",
      "INFO - Agent dqn, step 1242, rl-loss: 0.44504255056381226[1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 1 1]\n",
      "INFO - Agent dqn, step 1243, rl-loss: 0.4718800187110901[0 1 0 0 0 0 1 1 0 0 0 1 0 1 0 1 0 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1]\n",
      "INFO - Agent dqn, step 1244, rl-loss: 0.5997903943061829[0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 1 0 1 1 1 0]\n",
      "INFO - Agent dqn, step 1245, rl-loss: 0.621110200881958[1 0 1 0 1 1 0 1 0 1 0 0 0 0 1 0 1 1 0 0 1 0 1 0 1 1 0 1 0 1 1 0]\n",
      "INFO - Agent dqn, step 1246, rl-loss: 0.5012781620025635[0 1 1 1 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 1 0 1 1 1 0 1 1]\n",
      "INFO - Agent dqn, step 1247, rl-loss: 0.42666107416152954[0 1 1 1 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 1]\n",
      "INFO - Agent dqn, step 1248, rl-loss: 0.4402714669704437[0 1 1 0 0 1 0 1 0 1 0 0 1 1 1 0 1 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1]\n",
      "INFO - Agent dqn, step 1249, rl-loss: 0.45789971947669983[0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 0 1 1 0 0 0 1 1 0 1 0]\n",
      "INFO - Agent dqn, step 1250, rl-loss: 0.4890122711658478[1 1 1 1 0 1 0 1 0 0 0 0 1 1 0 1 1 1 0 1 0 0 1 1 1 1 0 0 1 1 0 0]\n",
      "INFO - Agent dqn, step 1251, rl-loss: 0.4950696527957916[1 1 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1]\n",
      "INFO - Agent dqn, step 1252, rl-loss: 0.4011320471763611[1 1 0 0 1 0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 0 1 0 1 0 0 0 0 1 1 1 1]\n",
      "INFO - Agent dqn, step 1253, rl-loss: 0.5007596015930176[1 1 1 1 0 0 1 0 1 0 1 1 0 0 0 0 1 1 1 0 1 0 0 1 1 0 0 0 1 1 0 0]\n",
      "INFO - Agent dqn, step 1254, rl-loss: 0.5479327440261841[1 0 0 1 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 1 1 1 0 0 1]\n",
      "INFO - Agent dqn, step 1255, rl-loss: 0.5057319402694702\n",
      "----------------------------------------\n",
      "  timestep     |  1255\n",
      "  reward       |  -0.141\n",
      "----------------------------------------\n",
      "[1 0 0 0 0 0 1 0 1 1 0 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 0 0 1 0]\n",
      "INFO - Agent dqn, step 1256, rl-loss: 0.6023579239845276[0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 1 1 0 1 0 1]\n",
      "INFO - Agent dqn, step 1257, rl-loss: 0.39793816208839417[1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1]\n",
      "INFO - Agent dqn, step 1258, rl-loss: 0.4914414584636688[0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 0]\n",
      "INFO - Agent dqn, step 1259, rl-loss: 0.46895867586135864[0 1 1 0 1 0 1 1 1 1 0 1 0 0 0 1 1 0 1 1 1 0 1 0 0 0 0 1 1 0 0 0]\n",
      "INFO - Agent dqn, step 1260, rl-loss: 0.551476776599884[0 0 0 0 1 0 1 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 0 0]\n",
      "INFO - Agent dqn, step 1261, rl-loss: 0.47272276878356934[0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1]\n",
      "INFO - Agent dqn, step 1262, rl-loss: 0.4288743734359741[0 0 0 1 1 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1]\n",
      "INFO - Agent dqn, step 1263, rl-loss: 0.4625927805900574[0 1 1 0 0 0 0 0 0 1 1 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 0 0]\n",
      "INFO - Agent dqn, step 1264, rl-loss: 0.5550107359886169[1 1 1 1 0 1 1 0 0 0 1 0 1 0 1 0 0 1 0 0 0 1 0 1 0 1 1 0 1 1 0 0]\n",
      "INFO - Agent dqn, step 1265, rl-loss: 0.5223368406295776[1 0 1 0 0 1 1 0 1 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1]\n",
      "INFO - Agent dqn, step 1266, rl-loss: 0.6620456576347351[0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1]\n",
      "INFO - Agent dqn, step 1267, rl-loss: 0.508476972579956[0 1 0 1 0 1 0 0 1 0 1 1 0 0 1 1 0 0 0 0 1 1 1 0 1 0 0 1 0 0 1 1]\n",
      "INFO - Agent dqn, step 1268, rl-loss: 0.4229850172996521[1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0 1 0 1]\n",
      "INFO - Agent dqn, step 1269, rl-loss: 0.6206348538398743[0 1 0 0 0 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 0 1]\n",
      "INFO - Agent dqn, step 1270, rl-loss: 0.5247557163238525[1 1 0 1 0 1 1 0 0 0 1 0 0 0 0 1 1 0 1 1 1 1 0 1 0 0 1 1 1 1 0 0]\n",
      "INFO - Agent dqn, step 1271, rl-loss: 0.42449894547462463[0 0 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 1 1 0 1 0 1 1 1 1]\n",
      "INFO - Agent dqn, step 1272, rl-loss: 0.4007251262664795[1 1 0 1 1 1 0 0 1 1 1 0 0 1 0 0 1 1 0 1 0 1 0 0 0 1 1 0 0 1 0 1]\n",
      "INFO - Agent dqn, step 1273, rl-loss: 0.4997040629386902[0 1 1 1 0 1 0 1 0 0 1 0 1 1 1 1 0 1 1 1 1 0 0 0 0 0 1 0 0 0 1 0]\n",
      "INFO - Agent dqn, step 1274, rl-loss: 0.46526098251342773[0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0]\n",
      "INFO - Agent dqn, step 1275, rl-loss: 0.5336059927940369[1 1 0 0 1 0 1 0 1 0 1 0 1 1 1 0 0 0 0 0 0 1 1 0 0 0 1 1 0 0 0 0]\n",
      "INFO - Agent dqn, step 1276, rl-loss: 0.342307448387146[1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 0 0 1 1 0]\n",
      "INFO - Agent dqn, step 1277, rl-loss: 0.5386248230934143[1 0 0 1 0 1 1 0 0 1 0 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 0 0 1 1 0 0]\n",
      "INFO - Agent dqn, step 1278, rl-loss: 0.39313170313835144[1 0 0 0 1 0 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1]\n",
      "INFO - Agent dqn, step 1279, rl-loss: 0.6638740301132202[1 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0]\n",
      "INFO - Agent dqn, step 1280, rl-loss: 0.4177723228931427[1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 0 0 0 0]\n",
      "INFO - Agent dqn, step 1281, rl-loss: 0.3448420464992523[0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 0 1 0 0 0 1 1 0 0 0 1 1 1]\n",
      "INFO - Agent dqn, step 1282, rl-loss: 0.4376915395259857[1 1 0 1 0 0 1 1 0 0 0 0 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 1 0 0 1 1]\n",
      "INFO - Agent dqn, step 1283, rl-loss: 0.4754652976989746[1 1 1 1 1 1 0 0 0 0 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 0]\n",
      "INFO - Agent dqn, step 1284, rl-loss: 0.5983725190162659"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d7984a9fecf0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;31m# Feed transitions into agent memory, and train the agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mts\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrajectories\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m# Evaluate the performance. Play with random agents.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DEV/AdvancedMachineLearningCSC722/rlcard/rlcard/agents/dqn_agent_pytorch.py\u001b[0m in \u001b[0;36mfeed\u001b[0;34m(self, ts)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_t\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_memory_init_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtmp\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtmp\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DEV/AdvancedMachineLearningCSC722/rlcard/rlcard/agents/dqn_agent_pytorch.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0mnext_state_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mdone_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         ) = self.memory.sample()\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "''' An example of learning a Deep-Q Agent on Leduc Holdem\n",
    "'''\n",
    "import torch\n",
    "import os\n",
    "\n",
    "import rlcard\n",
    "from rlcard.agents import DQNAgentPytorch as DQNAgent\n",
    "from rlcard.agents import RandomAgent\n",
    "from rlcard.utils import set_global_seed, tournament\n",
    "from rlcard.utils import Logger\n",
    "\n",
    "# Make environment\n",
    "env = rlcard.make('blackjack', config={'seed': 0})\n",
    "eval_env = rlcard.make('blackjack', config={'seed': 0})\n",
    "\n",
    "# Set the iterations numbers and how frequently we evaluate the performance\n",
    "evaluate_every = 100\n",
    "evaluate_num = 1000\n",
    "episode_num = 100000\n",
    "\n",
    "# The intial memory size\n",
    "memory_init_size = 1000\n",
    "\n",
    "# Train the agent every X steps\n",
    "train_every = 1\n",
    "\n",
    "# The paths for saving the logs and learning curves\n",
    "log_dir = './experiments/limit_holdem_dqn_result/'\n",
    "\n",
    "# Set a global seed\n",
    "set_global_seed(0)\n",
    "\n",
    "agent = DQNAgent(scope='dqn',\n",
    "                 action_num=env.action_num,\n",
    "                 replay_memory_init_size=memory_init_size,\n",
    "                 train_every=train_every,\n",
    "                 state_shape=env.state_shape,\n",
    "                 mlp_layers=[128, 128],\n",
    "                 device=torch.device('cpu'))\n",
    "random_agent = RandomAgent(action_num=eval_env.action_num)\n",
    "env.set_agents([agent, random_agent])\n",
    "eval_env.set_agents([agent, random_agent])\n",
    "\n",
    "# Init a Logger to plot the learning curve\n",
    "logger = Logger(log_dir)\n",
    "\n",
    "for episode in range(episode_num):\n",
    "\n",
    "    # Generate data from the environment\n",
    "    trajectories, _ = env.run(is_training=True)\n",
    "\n",
    "    # Feed transitions into agent memory, and train the agent\n",
    "    for ts in trajectories[0]:\n",
    "        agent.feed(ts)\n",
    "\n",
    "    # Evaluate the performance. Play with random agents.\n",
    "    if episode % evaluate_every == 0:\n",
    "        logger.log_performance(env.timestep, tournament(eval_env, evaluate_num)[0])\n",
    "\n",
    "# Close files in the logger\n",
    "logger.close_files()\n",
    "\n",
    "# Plot the learning curve\n",
    "logger.plot('DQN')\n",
    "\n",
    "# Save model\n",
    "save_dir = 'models/leduc_holdem_dqn_pytorch'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "state_dict = agent.get_state_dict()\n",
    "print(state_dict. keys())\n",
    "torch.save(state_dict, os.path.join(save_dir, 'model.pth'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">> Blackjack human agent\n",
      ">> Start a new game\n",
      "\n",
      "=============   Dealer Hand   ===============\n",
      "┌─────────┐\n",
      "│A        │\n",
      "│         │\n",
      "│         │\n",
      "│    ♥    │\n",
      "│         │\n",
      "│         │\n",
      "│        A│\n",
      "└─────────┘\n",
      "===============   Player 0 Hand   ===============\n",
      "┌─────────┐   ┌─────────┐\n",
      "│4        │   │9        │\n",
      "│         │   │         │\n",
      "│         │   │         │\n",
      "│    ♦    │   │    ♣    │\n",
      "│         │   │         │\n",
      "│         │   │         │\n",
      "│        4│   │        9│\n",
      "└─────────┘   └─────────┘\n",
      "===============   Player 1 Hand   ===============\n",
      "┌─────────┐   ┌─────────┐\n",
      "│10       │   │J        │\n",
      "│         │   │         │\n",
      "│         │   │         │\n",
      "│    ♠    │   │    ♣    │\n",
      "│         │   │         │\n",
      "│         │   │         │\n",
      "│       01│   │        J│\n",
      "└─────────┘   └─────────┘\n",
      "\n",
      "=========== Actions You Can Choose ===========\n",
      "0: hit, 1: stand\n",
      "\n",
      ">> Player 0 chooses stand\n",
      ">> Player 1 chooses stand\n",
      "===============   Dealer hand   ===============\n",
      "┌─────────┐   ┌─────────┐\n",
      "│10       │   │A        │\n",
      "│         │   │         │\n",
      "│         │   │         │\n",
      "│    ♣    │   │    ♥    │\n",
      "│         │   │         │\n",
      "│         │   │         │\n",
      "│       01│   │        A│\n",
      "└─────────┘   └─────────┘\n",
      "===============   Player 0 Hand   ===============\n",
      "┌─────────┐   ┌─────────┐\n",
      "│4        │   │9        │\n",
      "│         │   │         │\n",
      "│         │   │         │\n",
      "│    ♦    │   │    ♣    │\n",
      "│         │   │         │\n",
      "│         │   │         │\n",
      "│        4│   │        9│\n",
      "└─────────┘   └─────────┘\n",
      "===============   Player 1 Hand   ===============\n",
      "┌─────────┐   ┌─────────┐\n",
      "│10       │   │J        │\n",
      "│         │   │         │\n",
      "│         │   │         │\n",
      "│    ♠    │   │    ♣    │\n",
      "│         │   │         │\n",
      "│         │   │         │\n",
      "│       01│   │        J│\n",
      "└─────────┘   └─────────┘\n",
      "===============     Result     ===============\n",
      "Player 0 lose 1 chip!\n",
      "\n",
      "Player 1 lose 1 chip!\n",
      "\n",
      ">> Start a new game\n",
      "\n",
      "=============   Dealer Hand   ===============\n",
      "┌─────────┐\n",
      "│J        │\n",
      "│         │\n",
      "│         │\n",
      "│    ♦    │\n",
      "│         │\n",
      "│         │\n",
      "│        J│\n",
      "└─────────┘\n",
      "===============   Player 0 Hand   ===============\n",
      "┌─────────┐   ┌─────────┐\n",
      "│6        │   │5        │\n",
      "│         │   │         │\n",
      "│         │   │         │\n",
      "│    ♠    │   │    ♥    │\n",
      "│         │   │         │\n",
      "│         │   │         │\n",
      "│        6│   │        5│\n",
      "└─────────┘   └─────────┘\n",
      "===============   Player 1 Hand   ===============\n",
      "┌─────────┐   ┌─────────┐\n",
      "│2        │   │6        │\n",
      "│         │   │         │\n",
      "│         │   │         │\n",
      "│    ♦    │   │    ♣    │\n",
      "│         │   │         │\n",
      "│         │   │         │\n",
      "│        2│   │        6│\n",
      "└─────────┘   └─────────┘\n",
      "\n",
      "=========== Actions You Can Choose ===========\n",
      "0: hit, 1: stand\n",
      "\n",
      ">> Player 0 chooses hit\n",
      ">> Player 1 chooses hit\n",
      "\n",
      "=============   Dealer Hand   ===============\n",
      "┌─────────┐\n",
      "│J        │\n",
      "│         │\n",
      "│         │\n",
      "│    ♦    │\n",
      "│         │\n",
      "│         │\n",
      "│        J│\n",
      "└─────────┘\n",
      "===============   Player 0 Hand   ===============\n",
      "┌─────────┐   ┌─────────┐   ┌─────────┐\n",
      "│6        │   │5        │   │9        │\n",
      "│         │   │         │   │         │\n",
      "│         │   │         │   │         │\n",
      "│    ♠    │   │    ♥    │   │    ♠    │\n",
      "│         │   │         │   │         │\n",
      "│         │   │         │   │         │\n",
      "│        6│   │        5│   │        9│\n",
      "└─────────┘   └─────────┘   └─────────┘\n",
      "===============   Player 1 Hand   ===============\n",
      "┌─────────┐   ┌─────────┐   ┌─────────┐\n",
      "│2        │   │6        │   │J        │\n",
      "│         │   │         │   │         │\n",
      "│         │   │         │   │         │\n",
      "│    ♦    │   │    ♣    │   │    ♠    │\n",
      "│         │   │         │   │         │\n",
      "│         │   │         │   │         │\n",
      "│        2│   │        6│   │        J│\n",
      "└─────────┘   └─────────┘   └─────────┘\n",
      "\n",
      "=========== Actions You Can Choose ===========\n",
      "0: hit, 1: stand\n",
      "\n",
      ">> Player 0 chooses hit\n",
      ">> Player 1 chooses hit\n",
      ">> Player 0 chooses stand\n",
      ">> Player 1 chooses stand\n",
      "===============   Dealer hand   ===============\n",
      "┌─────────┐   ┌─────────┐   ┌─────────┐\n",
      "│4        │   │J        │   │K        │\n",
      "│         │   │         │   │         │\n",
      "│         │   │         │   │         │\n",
      "│    ♠    │   │    ♦    │   │    ♦    │\n",
      "│         │   │         │   │         │\n",
      "│         │   │         │   │         │\n",
      "│        4│   │        J│   │        K│\n",
      "└─────────┘   └─────────┘   └─────────┘\n",
      "===============   Player 0 Hand   ===============\n",
      "┌─────────┐   ┌─────────┐   ┌─────────┐\n",
      "│6        │   │5        │   │9        │\n",
      "│         │   │         │   │         │\n",
      "│         │   │         │   │         │\n",
      "│    ♠    │   │    ♥    │   │    ♠    │\n",
      "│         │   │         │   │         │\n",
      "│         │   │         │   │         │\n",
      "│        6│   │        5│   │        9│\n",
      "└─────────┘   └─────────┘   └─────────┘\n",
      "===============   Player 1 Hand   ===============\n",
      "┌─────────┐   ┌─────────┐   ┌─────────┐\n",
      "│2        │   │6        │   │J        │\n",
      "│         │   │         │   │         │\n",
      "│         │   │         │   │         │\n",
      "│    ♦    │   │    ♣    │   │    ♠    │\n",
      "│         │   │         │   │         │\n",
      "│         │   │         │   │         │\n",
      "│        2│   │        6│   │        J│\n",
      "└─────────┘   └─────────┘   └─────────┘\n",
      "===============     Result     ===============\n",
      "Player 0 win 1 chip!\n",
      "\n",
      "Player 1 win 1 chip!\n",
      "\n",
      ">> Start a new game\n",
      "\n",
      "=============   Dealer Hand   ===============\n",
      "┌─────────┐\n",
      "│3        │\n",
      "│         │\n",
      "│         │\n",
      "│    ♦    │\n",
      "│         │\n",
      "│         │\n",
      "│        3│\n",
      "└─────────┘\n",
      "===============   Player 0 Hand   ===============\n",
      "┌─────────┐   ┌─────────┐\n",
      "│K        │   │2        │\n",
      "│         │   │         │\n",
      "│         │   │         │\n",
      "│    ♠    │   │    ♠    │\n",
      "│         │   │         │\n",
      "│         │   │         │\n",
      "│        K│   │        2│\n",
      "└─────────┘   └─────────┘\n",
      "===============   Player 1 Hand   ===============\n",
      "┌─────────┐   ┌─────────┐\n",
      "│5        │   │2        │\n",
      "│         │   │         │\n",
      "│         │   │         │\n",
      "│    ♦    │   │    ♦    │\n",
      "│         │   │         │\n",
      "│         │   │         │\n",
      "│        5│   │        2│\n",
      "└─────────┘   └─────────┘\n",
      "\n",
      "=========== Actions You Can Choose ===========\n",
      "0: hit, 1: stand\n",
      "\n",
      ">> Player 0 chooses stand\n",
      ">> Player 1 chooses hit\n",
      "\n",
      "=============   Dealer Hand   ===============\n",
      "┌─────────┐   ┌─────────┐\n",
      "│3        │   │6        │\n",
      "│         │   │         │\n",
      "│         │   │         │\n",
      "│    ♦    │   │    ♥    │\n",
      "│         │   │         │\n",
      "│         │   │         │\n",
      "│        3│   │        6│\n",
      "└─────────┘   └─────────┘\n",
      "===============   Player 0 Hand   ===============\n",
      "┌─────────┐   ┌─────────┐\n",
      "│K        │   │2        │\n",
      "│         │   │         │\n",
      "│         │   │         │\n",
      "│    ♠    │   │    ♠    │\n",
      "│         │   │         │\n",
      "│         │   │         │\n",
      "│        K│   │        2│\n",
      "└─────────┘   └─────────┘\n",
      "===============   Player 1 Hand   ===============\n",
      "┌─────────┐   ┌─────────┐   ┌─────────┐\n",
      "│5        │   │2        │   │Q        │\n",
      "│         │   │         │   │         │\n",
      "│         │   │         │   │         │\n",
      "│    ♦    │   │    ♦    │   │    ♣    │\n",
      "│         │   │         │   │         │\n",
      "│         │   │         │   │         │\n",
      "│        5│   │        2│   │        Q│\n",
      "└─────────┘   └─────────┘   └─────────┘\n",
      "\n",
      "=========== Actions You Can Choose ===========\n",
      "0: hit, 1: stand\n",
      "\n",
      ">> Player 0 chooses stand\n",
      ">> Player 1 chooses hit\n",
      ">> Player 0 chooses hit\n",
      ">> Player 1 chooses hit\n",
      "===============   Dealer hand   ===============\n",
      "┌─────────┐   ┌─────────┐   ┌─────────┐\n",
      "│K        │   │3        │   │6        │\n",
      "│         │   │         │   │         │\n",
      "│         │   │         │   │         │\n",
      "│    ♥    │   │    ♦    │   │    ♥    │\n",
      "│         │   │         │   │         │\n",
      "│         │   │         │   │         │\n",
      "│        K│   │        3│   │        6│\n",
      "└─────────┘   └─────────┘   └─────────┘\n",
      "===============   Player 0 Hand   ===============\n",
      "┌─────────┐   ┌─────────┐   ┌─────────┐\n",
      "│K        │   │2        │   │7        │\n",
      "│         │   │         │   │         │\n",
      "│         │   │         │   │         │\n",
      "│    ♠    │   │    ♠    │   │    ♦    │\n",
      "│         │   │         │   │         │\n",
      "│         │   │         │   │         │\n",
      "│        K│   │        2│   │        7│\n",
      "└─────────┘   └─────────┘   └─────────┘\n",
      "===============   Player 1 Hand   ===============\n",
      "┌─────────┐   ┌─────────┐   ┌─────────┐   ┌─────────┐\n",
      "│5        │   │2        │   │Q        │   │8        │\n",
      "│         │   │         │   │         │   │         │\n",
      "│         │   │         │   │         │   │         │\n",
      "│    ♦    │   │    ♦    │   │    ♣    │   │    ♠    │\n",
      "│         │   │         │   │         │   │         │\n",
      "│         │   │         │   │         │   │         │\n",
      "│        5│   │        2│   │        Q│   │        8│\n",
      "└─────────┘   └─────────┘   └─────────┘   └─────────┘\n",
      "===============     Result     ===============\n",
      "Player 0 lose 1 chip!\n",
      "\n",
      "Player 1 lose 1 chip!\n",
      "\n",
      ">> Start a new game\n",
      "\n",
      "=============   Dealer Hand   ===============\n",
      "┌─────────┐\n",
      "│9        │\n",
      "│         │\n",
      "│         │\n",
      "│    ♦    │\n",
      "│         │\n",
      "│         │\n",
      "│        9│\n",
      "└─────────┘\n",
      "===============   Player 0 Hand   ===============\n",
      "┌─────────┐   ┌─────────┐\n",
      "│6        │   │J        │\n",
      "│         │   │         │\n",
      "│         │   │         │\n",
      "│    ♠    │   │    ♥    │\n",
      "│         │   │         │\n",
      "│         │   │         │\n",
      "│        6│   │        J│\n",
      "└─────────┘   └─────────┘\n",
      "===============   Player 1 Hand   ===============\n",
      "┌─────────┐   ┌─────────┐\n",
      "│J        │   │10       │\n",
      "│         │   │         │\n",
      "│         │   │         │\n",
      "│    ♠    │   │    ♦    │\n",
      "│         │   │         │\n",
      "│         │   │         │\n",
      "│        J│   │       01│\n",
      "└─────────┘   └─────────┘\n",
      "\n",
      "=========== Actions You Can Choose ===========\n",
      "0: hit, 1: stand\n",
      "\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-58bf40ee8aba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\">> Start a new game\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mtrajectories\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpayoffs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;31m# If the human does not take the final action, we need to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# print other players action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DEV/AdvancedMachineLearningCSC722/rlcard/rlcard/envs/env.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, is_training)\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0;31m# Agent plays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m                 \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mplayer_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mplayer_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DEV/AdvancedMachineLearningCSC722/rlcard/rlcard/agents/blackjack_human_agent.py\u001b[0m in \u001b[0;36meval_step\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mprobs\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0maction\u001b[0m \u001b[0mprobabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         '''\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_print_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_legal_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_record\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DEV/AdvancedMachineLearningCSC722/rlcard/rlcard/agents/blackjack_human_agent.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     26\u001b[0m         '''\n\u001b[1;32m     27\u001b[0m         \u001b[0m_print_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'raw_obs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'raw_legal_actions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'action_record'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'>> You choose action (integer): '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'legal_actions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Action illegel...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "source": [
    "\n",
    "import rlcard\n",
    "from rlcard.agents import RandomAgent as RandomAgent\n",
    "from rlcard.agents import BlackjackHumanAgent as HumanAgent\n",
    "from rlcard.utils.utils import print_card\n",
    "\n",
    "# Make environment and enable human mode\n",
    "# Set 'record_action' to True because we need it to print results\n",
    "player_num = 2\n",
    "env = rlcard.make('blackjack', config={'record_action': True, 'game_player_num': player_num})\n",
    "human_agent = HumanAgent(env.action_num)\n",
    "random_agent = RandomAgent(env.action_num)\n",
    "env.set_agents([human_agent, random_agent])\n",
    "\n",
    "print(\">> Blackjack human agent\")\n",
    "\n",
    "while (True):\n",
    "    print(\">> Start a new game\")\n",
    "\n",
    "    trajectories, payoffs = env.run(is_training=False)\n",
    "    # If the human does not take the final action, we need to\n",
    "    # print other players action\n",
    "\n",
    "    if len(trajectories[0]) != 0:\n",
    "        final_state = []\n",
    "        action_record = []\n",
    "        state = []\n",
    "        _action_list = []\n",
    "\n",
    "        for i in range(player_num):\n",
    "            final_state.append(trajectories[i][-1][-2])\n",
    "            state.append(final_state[i]['raw_obs'])\n",
    "\n",
    "        action_record.append(final_state[i]['action_record'])\n",
    "        for i in range(1, len(action_record) + 1):\n",
    "            _action_list.insert(0, action_record[-i])\n",
    "\n",
    "        for pair in _action_list[0]:\n",
    "            print('>> Player', pair[0], 'chooses', pair[1])\n",
    "\n",
    "    # Let's take a look at what the agent card is\n",
    "    print('===============   Dealer hand   ===============')\n",
    "    print_card(state[0]['state'][1])\n",
    "\n",
    "    for i in range(player_num):\n",
    "        print('===============   Player {} Hand   ==============='.format(i))\n",
    "        print_card(state[i]['state'][0])\n",
    "\n",
    "    print('===============     Result     ===============')\n",
    "    for i in range(player_num):\n",
    "        if payoffs[i] == 1:\n",
    "            print('Player {} win {} chip!'.format(i, payoffs[i]))\n",
    "        elif payoffs[i] == 0:\n",
    "            print('Player {} is tie'.format(i))\n",
    "        else:\n",
    "            print('Player {} lose {} chip!'.format(i, -payoffs[i]))\n",
    "        print('')\n",
    "\n",
    "    input(\"Press any key to continue...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0"
   ]
  }
 ]
}